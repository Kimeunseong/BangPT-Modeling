{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6470caba",
   "metadata": {},
   "source": [
    "# 2023-05-15\n",
    "\n",
    "## 전체 시퀀스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64000b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 필요한 모듈 로드\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87fcf27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 완료\n",
      "2번 완료\n",
      "3번 완료\n",
      "4번 완료\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2ff0c5900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "5번 완료\n",
      "6번 완료\n",
      "7번 완료\n",
      "8번 완료\n",
      "9번 완료\n"
     ]
    }
   ],
   "source": [
    "input_video_path = '/Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/assets/video.mp4' # 인풋 동영상 경로\n",
    "split_video_path = '/Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/test_video/output_segment'\n",
    "\n",
    "\n",
    "# 1. full video를 5개로 분할하여 경로(split_video_path)에 저장한다.\n",
    "video_split(input_video_path, split_video_path, 5)\n",
    "print('1번 완료')\n",
    "\n",
    "# 2. 각 동영상을 15장의 이미지셋으로 변환하여 어레이로 저장한다.(images_list00, images_list01, ..., images_list04)\n",
    "for i in range(5):\n",
    "    video_path = f'{split_video_path}_{i+1}.mp4'\n",
    "    globals()[f'images_list0{i}'] = video_to_images(15, video_path)\n",
    "print('2번 완료')\n",
    "\n",
    "# 3. 이미지에서 스켈레톤 좌표를 추출하여 df로 저장한다.(df00, df01, df02, df03, df04, shape = (15, 22))\n",
    "for n in range(5):\n",
    "    total_skeleton = []\n",
    "    for j in range(15):\n",
    "        temp_img = img_to_skeleton(globals()[f'images_list0{n}'][j])\n",
    "        total_skeleton.append(temp_img) # 여기서 스켈레톤 추출\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(total_skeleton)):\n",
    "        temp = np.array(total_skeleton[i])\n",
    "        temp = temp.flatten()\n",
    "        df[i] = temp\n",
    "    globals()[f'df0{n}'] = df.T\n",
    "print('3번 완료')\n",
    "\n",
    "# 4. 모델 인풋에 맞게 shape을 조정한다. inputs shape = (5,22), outputs shape = (5, 308)\n",
    "inputs, outputs = shape_control(df00, df01, df02, df03, df04)\n",
    "print('4번 완료')\n",
    "\n",
    "# 5. 모델 예측 실행하여 pred에 저장한다. pred.shape = (5, 308)\n",
    "model = load_model('./learned_models/model_0520_132_ver1.h5')\n",
    "pred = model.predict(inputs)\n",
    "print('5번 완료')\n",
    "\n",
    "# 6. pred와 ture를 비교하여 mse를 계산한다.\n",
    "lst_loss = []\n",
    "for i in range(len(pred)):\n",
    "    loss = str(mse_loss(outputs[i], pred[i]))[10:30]\n",
    "    lst_loss.append(loss) \n",
    "print('6번 완료')\n",
    "\n",
    "# 7. 5회의 스쿼트에 대한 100점 만점 기준 점수와 최종 점수를 제공한다.\n",
    "score_100 = convert_to_score(lst_loss)\n",
    "score_100_total = sum(convert_to_score(lst_loss))//5\n",
    "print('7번 완료')\n",
    "\n",
    "# 8. 최고, 최저 랩스의 번호와 해당 동영상의 경로를 찾는다.\n",
    "best_rep, worst_rep = find_max_min_index(score_100)\n",
    "best_rep_video = f'{split_video_path}_{best_rep+1}.mp4'\n",
    "worst_rep_video = f'{split_video_path}_{worst_rep+1}.mp4'\n",
    "print('8번 완료')\n",
    "\n",
    "# 9. 운동에 대한 피드백을 보여준다.\n",
    "feedback = oneLineFeedback(score_100_total)\n",
    "print('9번 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d090dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_100 : [93.45764248764415, 95.47488746568605, 96.77296258585773, 96.17001506101084, 93.33050008232766]\n",
      "score_100_total : 95.0\n",
      "best_rep, worst_rep : (4, 2)\n",
      "best_rep_video : /Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/test_video/output_segment_5.mp4\n",
      "worst_rep_video : /Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/test_video/output_segment_3.mp4\n",
      "feedback : 멋져요! 운동 자세가 상당히 훌륭합니다. 자세의 안정성과 균형을 유지하며, 기본적인 운동 동작을 정확하게 수행하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "\n",
    "print(f'score_100 : {score_100}')\n",
    "print(f'score_100_total : {score_100_total}')\n",
    "print(f'best_rep, worst_rep : {best_rep, worst_rep}')\n",
    "print(f'best_rep_video : {best_rep_video}')\n",
    "print(f'worst_rep_video : {worst_rep_video}')\n",
    "print(f'feedback : {feedback}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6fadb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.049739926330922515',\n",
       " '0.07961403288227067,',\n",
       " '0.11726666240974239,',\n",
       " '0.08753686678974107,',\n",
       " '0.07388163598762684,']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd9e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. full video를 5개로 분할하여 경로(split_video_path)에 저장한다.\n",
    "\n",
    "import cv2\n",
    "\n",
    "def video_split(input_file, output_prefix, segment_count):\n",
    "    # 동영상 파일 열기\n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "\n",
    "    # 동영상 속성 가져오기\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # 세그먼트 길이 계산\n",
    "    segment_length_frames = total_frames // segment_count\n",
    "\n",
    "    # 저장할 세그먼트 번호 초기화\n",
    "    segment_number = 1\n",
    "\n",
    "    frame_count = 0\n",
    "    while segment_number <= segment_count:\n",
    "        # 동영상의 현재 프레임 설정\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "\n",
    "        # 세그먼트 파일 이름 설정\n",
    "        output_file = f\"{output_prefix}_{segment_number}.mp4\"\n",
    "\n",
    "        # 세그먼트 파일 생성\n",
    "        out = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n",
    "\n",
    "        # 세그먼트 길이만큼 프레임 저장\n",
    "        for i in range(segment_length_frames):\n",
    "            # 동영상의 현재 프레임 읽기\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # 프레임 저장\n",
    "            out.write(frame)\n",
    "\n",
    "        # 세그먼트 파일 닫기\n",
    "        out.release()\n",
    "\n",
    "        # 다음 세그먼트 번호로 이동\n",
    "        segment_number += 1\n",
    "\n",
    "        # 다음 세그먼트의 시작 프레임 설정\n",
    "        frame_count += segment_length_frames\n",
    "\n",
    "    # 동영상 파일 닫기\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba311b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 각 동영상을 15장의 이미지셋으로 변환하여 어레이로 저장한다.\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def video_to_images(N, video_PATH) :\n",
    "    \n",
    "    # 1. 동영상의 모든 프레임을 리스트(temp_list)에 저장하기\n",
    "    temp_list = []\n",
    "    video = cv2.VideoCapture(video_PATH)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    while(video.isOpened()):\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if ret:\n",
    "            temp_list.append(frame)\n",
    "\n",
    "    video.release()\n",
    "    \n",
    "    # 2. 건너 뛸 간격(skip)을 계산해서 넘파이 어레이로 저장하기\n",
    "    images_list = []\n",
    "    skip = len(temp_list) / N\n",
    "    cnt = 0\n",
    "    for i in range(len(temp_list)):\n",
    "        if i == np.floor(skip*cnt):\n",
    "            images_list.append(temp_list[i])\n",
    "            cnt += 1\n",
    "    images_array = np.array(images_list)\n",
    "    \n",
    "    return images_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58e4a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[1361]: Class CaptureDelegate is implemented in both /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x28b7765a0) and /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x28de1c860). One of the two will be used. Which one is undefined.\n",
      "objc[1361]: Class CVWindow is implemented in both /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x28b7765f0) and /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x284e30a68). One of the two will be used. Which one is undefined.\n",
      "objc[1361]: Class CVView is implemented in both /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x28b776618) and /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x284e30a90). One of the two will be used. Which one is undefined.\n",
      "objc[1361]: Class CVSlider is implemented in both /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x28b776640) and /Users/estar-kim/miniconda3/envs/tfgpu2/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x284e30ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "# 3. 이미지에서 스켈레톤 좌표를 추출하여 df로 저장한다.(df00, df01, df02, df03, df04, shape = (15, 22))\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def img_to_skeleton(img_array):\n",
    "    \n",
    "    # 모듈 로드\n",
    "    mp_pose = mp.solutions.pose\n",
    "    \n",
    "    # 이미지에서 프레임을 읽어온다.\n",
    "    image = img_array\n",
    "    \n",
    "    # HumanPose 모듈을 사용하여 스켈레톤을 추출한다.\n",
    "    with mp_pose.Pose(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as pose:\n",
    "        \n",
    "        # 추출된 스켈레톤을 저장할 리스트를 생성한다.\n",
    "        landmarks_list = []\n",
    "        \n",
    "        # 스켈레톤을 추출한다.\n",
    "        results = pose.process(image)\n",
    "        landmark = results.pose_landmarks.landmark\n",
    "        \n",
    "        if len(landmark) == 33: # 33보다 적거나 많다는 것은 잘못된 추출이다.\n",
    "            \n",
    "            # 필요한 관절 좌표만 선별적으로 저장한다.\n",
    "            need = [0, 11, 12, 23, 24, 25, 26, 27, 28, 31, 32]\n",
    "            for i in need:\n",
    "                # 스켈레톤의 좌표를 저장한다.\n",
    "                x = round(landmark[i].x, 5)\n",
    "                y = round(landmark[i].y, 5)\n",
    "                landmarks_list.append((x, y)) # x,y좌표만 저장한다.\n",
    "                \n",
    "        else:\n",
    "            return 'There is a problem.'\n",
    "        \n",
    "    return landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52a07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 모델 인풋에 맞게 shape을 조정한다. inputs shape = (5,22), outputs shape = (5, 308)\n",
    "\n",
    "def shape_control(df00, df01, df02, df03, df04):\n",
    "    arrX = []\n",
    "    arrY = []\n",
    "\n",
    "    for i in range(5):\n",
    "        tempX = globals()[f'df0{i}'].iloc[0]\n",
    "        tempY = globals()[f'df0{i}'].iloc[1:]\n",
    "        tempX = np.array(tempX)\n",
    "        tempY = np.array(tempY)\n",
    "        tempY = tempY.flatten() # 일차원 넘파이 배열로 변경\n",
    "        arrX.append(tempX)\n",
    "        arrY.append(tempY)\n",
    "\n",
    "    inputs = np.array(arrX)\n",
    "    outputs = np.array(arrY)\n",
    "\n",
    "#     print(f'inputs.shape = {inputs.shape}, outputs.shape = {outputs.shape}')\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69face4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. mse 계산 함수\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# mse 계산\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    loss = tf.math.reduce_mean(tf.math.square(err))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57232afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 점수로 환산\n",
    "\n",
    "def convert_to_score(score):\n",
    "    score_clean = [float(s.strip().rstrip(',')) for s in score]\n",
    "    score_100 = []\n",
    "    for i in score_clean:\n",
    "        score_100.append(100 - i*1000)\n",
    "    return score_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9858060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. best, worst reps 계산\n",
    "\n",
    "def find_max_min_index(lst):\n",
    "    min_value = min(lst)\n",
    "    max_value = max(lst)\n",
    "    min_index = lst.index(min_value)\n",
    "    max_index = lst.index(max_value)\n",
    "    return min_index, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67dad6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 한 문장 피드백 제공\n",
    "\n",
    "def oneLineFeedback(tot_score):\n",
    "    if 96 <= tot_score:\n",
    "        return '대단해요! 거의 트레이너 수준입니다! 운동 자세가 매우 정확하고 균형도 잘 잡혀 있습니다. 전문가 수준의 자세를 유지하고 있으며, 안정적이고 효과적인 운동을 하고 있습니다.'\n",
    "    elif 75 <= tot_score <= 95:\n",
    "        return '멋져요! 운동 자세가 상당히 훌륭합니다. 자세의 안정성과 균형을 유지하며, 기본적인 운동 동작을 정확하게 수행하고 있습니다.'\n",
    "    elif 65 <= tot_score <= 74:\n",
    "        return '잘 하셨어요! 운동 자세가 괜찮습니다. 기본적인 운동 동작을 어느 정도 정확하게 수행하고 있으며, 자세의 안정성도 일정 수준을 유지하고 있습니다. 조금 더 정확한 동작을 위해 세심한 주의가 필요합니다.'\n",
    "    elif 50 <= tot_score <= 64:\n",
    "        return '아쉬워요! 기본적인 운동 동작을 대체로 잘 수행하고 있지만, 몇몇 부분에서 개선이 필요합니다. 조금 더 정확하고 자세한 동작을 위해 주의와 연습이 더욱 필요합니다.'\n",
    "    else:\n",
    "        return '앗, 야생의 헬린이가 나타났다! 운동 자세가 아직 미흡한 부분이 많습니다. 자세의 안정성과 정확성을 향상시키기 위해 더 많은 연습과 주의가 필요합니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "563c99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 완료\n",
      "2번 완료\n",
      "3번 완료\n",
      "4번 완료\n",
      "Metal device set to: Apple M2 Pro\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "5번 완료\n",
      "6번 완료\n",
      "\n",
      "best reps : 2번째 영상\n",
      "worst reps : 4번째 영상\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 01:52:34.247767: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# 다른 데이터로 한 번 더 해봄\n",
    "\n",
    "input_video_path = './data/total_0513/4_test_data/test_video01.mov'\n",
    "split_video_path = './testing/1_video/output_segment'\n",
    "\n",
    "\n",
    "# 1. full video를 5개로 분할하여 경로(split_video_path)에 저장한다.\n",
    "video_split(input_video_path, split_video_path, 5)\n",
    "print('1번 완료')\n",
    "\n",
    "# 2. 각 동영상을 15장의 이미지셋으로 변환하여 어레이로 저장한다.(images_list00, images_list01, ..., images_list04)\n",
    "for i in range(5):\n",
    "    video_path = f'./testing/1_video/output_segment_{i+1}.mp4'\n",
    "    globals()[f'images_list0{i}'] = video_to_images(15, video_path)\n",
    "print('2번 완료')\n",
    "\n",
    "# 3. 이미지에서 스켈레톤 좌표를 추출하여 df로 저장한다.(df00, df01, df02, df03, df04, shape = (15, 22))\n",
    "for n in range(5):\n",
    "    total_skeleton = []\n",
    "    for j in range(15):\n",
    "        temp_img = img_to_skeleton(globals()[f'images_list0{n}'][j])\n",
    "        total_skeleton.append(temp_img) # 여기서 스켈레톤 추출\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(total_skeleton)):\n",
    "        temp = np.array(total_skeleton[i])\n",
    "        temp = temp.flatten()\n",
    "        df[i] = temp\n",
    "    globals()[f'df0{n}'] = df.T\n",
    "print('3번 완료')\n",
    "\n",
    "# 4. 모델 인풋에 맞게 shape을 조정한다. inputs shape = (5,22), outputs shape = (5, 308)\n",
    "inputs, outputs = shape_control(df00, df01, df02, df03, df04)\n",
    "print('4번 완료')\n",
    "\n",
    "# 5. 모델 예측 실행하여 pred에 저장한다. pred.shape = (5, 308)\n",
    "model = load_model('./learned_models/model_0517_113_ver1.h5')\n",
    "pred = model.predict(inputs)\n",
    "print('5번 완료')\n",
    "\n",
    "# 6. pred와 ture를 비교하여 mse를 계산하고 best, worst rep을 반환한다.\n",
    "lst_loss = []\n",
    "for i in range(len(pred)):\n",
    "    loss = str(mse_loss(outputs[i], pred[i]))[10:30]\n",
    "    lst_loss.append(loss) \n",
    "best = min(lst_loss)\n",
    "worst = max(lst_loss)\n",
    "print('6번 완료\\n')\n",
    "print(f'best reps : {lst_loss.index(best)+1}번째 영상')\n",
    "print(f'worst reps : {lst_loss.index(worst)+1}번째 영상')\n",
    "\n",
    "# 7. best, worst reps에 대한 동영상 보여주기\n",
    "# print(f'best reps path = {split_video_path} + ')\n",
    "\n",
    "\n",
    "# 8. 운동에 대한 피드백을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7633376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 완료\n",
      "2번 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3번 완료\n",
      "4번 완료\n",
      "Metal device set to: Apple M2 Pro\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "5번 완료\n",
      "6번 완료\n",
      "7번 완료\n",
      "8번 완료\n",
      "9번 완료\n",
      "score_100 : [69.91195581639693, 80.01681262997886, 77.63440197787709, 81.23050977069667, 45.64481344991454]\n",
      "score_100_total : 70.0\n",
      "best_rep, worst_rep : (4, 3)\n",
      "best_rep_video : /Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/test_video/output_segment_5.mp4\n",
      "worst_rep_video : /Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/test_video/output_segment_4.mp4\n",
      "feedback : 잘 하셨어요! 운동 자세가 괜찮습니다. 기본적인 운동 동작을 어느 정도 정확하게 수행하고 있으며, 자세의 안정성도 일정 수준을 유지하고 있습니다. 조금 더 정확한 동작을 위해 세심한 주의가 필요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 21:44:58.020123: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "input_video_path = '/Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/assets/video.mp4' # 인풋 동영상 경로\n",
    "split_video_path = '/Users/estar-kim/Desktop/2023/mju/캡스톤디자인/flask/practice4/static/test_video/output_segment'\n",
    "\n",
    "\n",
    "# 1. full video를 5개로 분할하여 경로(split_video_path)에 저장한다.\n",
    "video_split(input_video_path, split_video_path, 5)\n",
    "print('1번 완료')\n",
    "\n",
    "# 2. 각 동영상을 15장의 이미지셋으로 변환하여 어레이로 저장한다.(images_list00, images_list01, ..., images_list04)\n",
    "for i in range(5):\n",
    "    video_path = f'{split_video_path}_{i+1}.mp4'\n",
    "    globals()[f'images_list0{i}'] = video_to_images(15, video_path)\n",
    "print('2번 완료')\n",
    "\n",
    "# 3. 이미지에서 스켈레톤 좌표를 추출하여 df로 저장한다.(df00, df01, df02, df03, df04, shape = (15, 22))\n",
    "for n in range(5):\n",
    "    total_skeleton = []\n",
    "    for j in range(15):\n",
    "        temp_img = img_to_skeleton(globals()[f'images_list0{n}'][j])\n",
    "        total_skeleton.append(temp_img) # 여기서 스켈레톤 추출\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(total_skeleton)):\n",
    "        temp = np.array(total_skeleton[i])\n",
    "        temp = temp.flatten()\n",
    "        df[i] = temp\n",
    "    globals()[f'df{n}'] = df.T\n",
    "print('3번 완료')\n",
    "\n",
    "# 4. 모델 인풋에 맞게 shape을 조정한다. inputs shape = (5,22), outputs shape = (5, 308)\n",
    "\n",
    "# 4. 모델 인풋에 맞게 shape을 조정한다. inputs shape = (5,22), outputs shape = (5, 308)\n",
    "# import numpy as np\n",
    "def shape_control(df0, df1, df2, df3, df4):\n",
    "    arrX = []\n",
    "    arrY = []\n",
    "    for i in range(5):\n",
    "        tempX = globals()[f'df{i}'].iloc[0]\n",
    "        tempY = globals()[f'df{i}'].iloc[1:]\n",
    "        tempX = np.array(tempX)\n",
    "        tempY = np.array(tempY)\n",
    "        tempY = tempY.flatten() # 일차원 넘파이 배열로 변경\n",
    "        arrX.append(tempX)\n",
    "        arrY.append(tempY)\n",
    "    inputs = np.array(arrX)\n",
    "    outputs = np.array(arrY)\n",
    "    return inputs, outputs\n",
    "\n",
    "inputs, outputs = shape_control(df0, df1, df2, df3, df4)\n",
    "print('4번 완료')\n",
    "\n",
    "# 5. 모델 예측 실행하여 pred에 저장한다. pred.shape = (5, 308)\n",
    "model = load_model('/Users/estar-kim/Desktop/2023/mju/캡스톤디자인/model/learned_models/model_0520_132_ver1.h5')\n",
    "pred = model.predict(inputs)\n",
    "print('5번 완료')\n",
    "\n",
    "# 6. pred와 ture를 비교하여 mse를 계산한다.\n",
    "lst_loss = []\n",
    "for i in range(len(pred)):\n",
    "    loss = str(mse_loss(outputs[i], pred[i]))[10:30]\n",
    "    lst_loss.append(loss) \n",
    "print('6번 완료')\n",
    "\n",
    "# 7. 5회의 스쿼트에 대한 100점 만점 기준 점수와 최종 점수를 제공한다.\n",
    "score_100 = convert_to_score(lst_loss)\n",
    "score_100_total = sum(convert_to_score(lst_loss))//5\n",
    "print('7번 완료')\n",
    "\n",
    "# 8. 최고, 최저 랩스의 번호와 해당 동영상의 경로를 찾는다.\n",
    "best_rep, worst_rep = find_max_min_index(score_100)\n",
    "best_rep_video = f'{split_video_path}_{best_rep+1}.mp4'\n",
    "worst_rep_video = f'{split_video_path}_{worst_rep+1}.mp4'\n",
    "print('8번 완료')\n",
    "\n",
    "# 9. 운동에 대한 피드백을 보여준다.\n",
    "feedback = oneLineFeedback(score_100_total)\n",
    "print('9번 완료')\n",
    "\n",
    "\n",
    "# ++++++ 결과 확인 ++++++\n",
    "print(f'score_100 : {score_100}')\n",
    "print(f'score_100_total : {score_100_total}')\n",
    "print(f'best_rep, worst_rep : {best_rep, worst_rep}')\n",
    "print(f'best_rep_video : {best_rep_video}')\n",
    "print(f'worst_rep_video : {worst_rep_video}')\n",
    "print(f'feedback : {feedback}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4488aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.000861725359677880',\n",
       " '0.000712028704872278',\n",
       " '0.001091081371595408',\n",
       " '0.001611983767425861',\n",
       " '0.000843242250506719']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 김은성 스쿼트 영상\n",
    "lst_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdb852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
